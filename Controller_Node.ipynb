{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm \n",
    "from time_tracker import time_tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = time_tracker(interval = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets the name of the saved data, based on device and resource state, and the number of models to create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = '64bit_WIFI'\n",
    "rpi_name = 'RPi4_1000'\n",
    "num_of_models = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on Openmined's course: \n",
    "https://courses.openmined.org/courses/foundations-of-private-computation\n",
    "\n",
    "The code for the Controller used in course can be found here: \n",
    "https://github.com/OpenMined/courses/blob/foundations-of-private-computation/federated-learning/duet_mnist/MNIST_Syft_Data_Scientist.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duet_RPi8_1500 = sy.join_duet(target_id=\"438be5a282e90f571b435feae2d0b648\", network_url=\"http://ec2-18-218-7-180.us-east-2.compute.amazonaws.com:5000\")\n",
    "duet = sy.duet(\"ab4caad04fd608d7b66f3331c7b12885\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_torch = duet.torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression Model Dimensions (each image is 8x8, with 10 labeles, and 1437 samples)\n",
    "in_dim = 64\n",
    "out_dim = 10\n",
    "n_samples = 1437\n",
    "\n",
    "#LR Model traning Parameters\n",
    "learning_rate = 0.01\n",
    "\n",
    "iteration = 100\n",
    "\n",
    "#Arrays required to compute RCoin Values\n",
    "\n",
    "RCoins = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyNet(sy.Module):\n",
    "    def __init__(self, torch_ref):\n",
    "        super(SyNet, self).__init__(torch_ref = torch_ref)\n",
    "        self.layer1 = self.torch_ref.nn.Linear(in_dim, 128)\n",
    "        self.layer2 = self.torch_ref.nn.Linear(128, 256)\n",
    "        self.layer3 = self.torch_ref.nn.Linear(256, 50)\n",
    "        self.dropout1 = self.torch_ref.nn.Dropout(0.25)\n",
    "        self.out = self.torch_ref.nn.Linear(50, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #with profiler_ref.record_function(\"Forward Pass\"):\n",
    "        x = self.torch_ref.nn.functional.relu(self.layer1(x))\n",
    "        x = self.torch_ref.nn.functional.relu(self.layer2(x))\n",
    "        x = self.torch_ref.nn.functional.relu(self.layer3(x))\n",
    "        output = self.torch_ref.nn.functional.log_softmax(self.out(x), dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterations, model, torch_ref, optim, data_ptr, target_ptr, monitor): #profiler_ref\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        iter_start_time = time.time()\n",
    "        optim.zero_grad()\n",
    "        #with profiler_ref.profile(profiler_memory=True) as prof:\n",
    "        output = model(data_ptr)\n",
    "\n",
    "        # nll_loss = negative log-liklihood loss\n",
    "        loss = torch_ref.nn.functional.nll_loss(output, target_ptr.long())\n",
    "\n",
    "        loss_item = loss.item()\n",
    "        \n",
    "        loss_value = loss_item.get( reason=\"To evaluate training progress\", request_block=True, timeout_secs=5 )\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            if loss_value is not None:\n",
    "                print(\"Train Epoch: {} loss {:.4}\".format(i, loss_value))\n",
    "            else:\n",
    "                print(\"Train Epoch: {}\".format(i))\n",
    "\n",
    "        losses.append(loss_value)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "        \n",
    "        x =  time.time() - iter_start_time\n",
    "        monitor.QOS =  x\n",
    "        monitor.QOS_list.append(x)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create the model and pass in our local copy of torch\n",
    "local_model = SyNet(torch) #profiler\n",
    "print(local_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receive Remote Data Pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duet.store.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ptr = duet.store[0]\n",
    "target_ptr = duet.store[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnsit = datasets.load_digits()\n",
    "X, y = mnsit.data, mnsit.target\n",
    "\n",
    "_, X_test, _, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)\n",
    "\n",
    "X_test = torch.FloatTensor(np.array(X_test))\n",
    "y_test = torch.FloatTensor(np.array(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_optims, remote_models = [], []\n",
    "for m in range(num_of_models):\n",
    "    \n",
    "    remote_models.append(local_model.send(duet))\n",
    "    remote_optims.append(remote_torch.optim.SGD(params=remote_models[m].parameters(), lr=learning_rate))\n",
    "    \n",
    "    \n",
    "print(remote_models)    \n",
    "print(remote_optims)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training and track QOS values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training on {} - {}\".format(rpi_name,test_name))\n",
    "startTimes, trainingTimes, training_losses = [], [], []      \n",
    "model_number = 1\n",
    "monitor.run_monitor_thread()\n",
    "\n",
    "for r in tqdm(range(num_of_models)):\n",
    "    monitor.task = 'Linear_Regression_Training'\n",
    "    monitor.model_num = model_number\n",
    "    print(\"Round number:\", r+1)\n",
    "    startTimes.append(time.time())\n",
    "    \n",
    "    training_loss  = train(iteration, remote_models[r], remote_torch, remote_optims[r], data_ptr, target_ptr, monitor) #remote_profiler\n",
    "    training_losses.append(training_loss)\n",
    "\n",
    "    trainingTimes.append(time.time() - startTimes[r])\n",
    "    print('Training time:', trainingTimes[-1],'for model', model_number)\n",
    "    model_number += 1\n",
    "    \n",
    "print('Done - Stopping Monitoring thread')\n",
    "time_data = monitor.stop_monitor_thread()\n",
    "monitor.model_num = None\n",
    "monitor.QOS = 0\n",
    "monitor.QOS_list = [0]\n",
    "monitor.task = 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_data.to_csv('data/{}/QOS_data_{}_{}.csv'.format(rpi_name,rpi_name,test_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rcoin "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receive Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_model(model):\n",
    "\n",
    "    local_model = model.get(\n",
    "        request_block=True,\n",
    "        reason=\"To run test and inference locally\",\n",
    "        timeout_secs=5,\n",
    "    )\n",
    "\n",
    "    return local_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_models = []\n",
    "for m in range(num_of_models):\n",
    "    local_models.append(get_local_model(remote_models[m]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of models on test data (section of data not used to train model on Worker deivce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 1\n",
    "accuracy = []\n",
    "for model in local_models:\n",
    "    correct = 0\n",
    "    preds = []\n",
    "    \n",
    "    print(\"Test Model\", count)\n",
    "    count += 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i in tqdm(range(len(X_test))):\n",
    "        \n",
    "            sample = X_test[i]\n",
    "            y_hat = model(sample.unsqueeze(0))\n",
    "            pred = y_hat.argmax().item()\n",
    "            \n",
    "            if y_test[i] == pred:\n",
    "                correct += 1\n",
    "                \n",
    "            preds.append(pred)\n",
    "            \n",
    "    accuracy.append(accuracy_score(y_test, preds))\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RCoinP(RCoins,num_of_models):\n",
    "\n",
    "    RCoinPs = []\n",
    "    RCoinPs.append(RCoins[0])\n",
    "    for i in range(1,num_of_models):\n",
    "        if i <= 4:\n",
    "            RCoinPs.append(RCoinPs[i-1] + RCoins[i])\n",
    "        else:\n",
    "            RCoinPs.append(RCoinPs[i-1] + RCoins[i] -  RCoins[i-5])\n",
    "            \n",
    "    return RCoinPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RCoins, training_loss_last = [], []\n",
    "for i in range(num_of_models):\n",
    "    training_loss_last.append(training_losses[i][-1])\n",
    "    RCoins.append( (iteration * accuracy[i]) / (training_losses[i][-1] * trainingTimes[i] * learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RCoins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RCoinPs = get_RCoinP(RCoins,num_of_models)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {'RCoin':RCoins,'RCoinP':RCoinPs, 'Accuracy':accuracy,'Training Losses': training_loss_last,'Training Times': trainingTimes}\n",
    "\n",
    "out = pd.DataFrame(out_dict)\n",
    "out.to_csv(\"data/{}/RCoin_{}_{}.csv\".format(rpi_name,rpi_name, test_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SyMatMull(torch_ref, monitor, n = 512, run_time = 300):\n",
    "    \n",
    "    #FLOPS_SCALE_FACTOR = 100_000_000_000\n",
    "    # this represents the number of floating point computations per matrix mult\n",
    "    ops = (n**3) + ((n-1)*(n**2)) # // n^2*(n-1) additions and n^3 mults\n",
    "    \n",
    "    monitor.task = 'Matrix_Multiplication'\n",
    "    matrix1 = torch.randn(size=(n,n), dtype=torch.float32)\n",
    "    matrix2 = torch.randn(size=(n,n), dtype=torch.float32)\n",
    "\n",
    "    matrix1_pointer = matrix1.send(duet, pointable=True)\n",
    "    matrix1_pointer = matrix2.send(duet, pointable=True)\n",
    "    print('Encryted matrixes sent')\n",
    "    \n",
    "    matrixOut = torch_ref.matmul(matrix1_pointer, matrix1_pointer)\n",
    "\n",
    "    #start = time.time()\n",
    "    #num_iters = 0\n",
    "\n",
    "    # runs the loop for the given amount of time\n",
    "    while (time.time()- run_time < start):\n",
    "        start_matMull = time.time()\n",
    "        matrixOut = torch_ref.matmul(matrix1_pointer, matrix2_pointer)\n",
    "        monitor.QOS = time.time() - start_matMull\n",
    "        monitor.QOS_list.append(time.time() - start_matMull)\n",
    "        \n",
    "        #other QOS measure\n",
    "        #num_iters += 1\n",
    "        #monitor.QOS = (num_iters * ops / ( time.time() - start )) / FLOPS_SCALE_FACTOR\n",
    "        #monitor.QOS_list.append(( num_iters * ops / ( time.time() - start ) ) / FLOPS_SCALE_FACTOR )\n",
    "\n",
    "    del matrix1, matrix2\n",
    "\n",
    "    \n",
    "\n",
    "    # calculates the rate at which the CPU performs floating point computations\n",
    "    #cpu_rate = num_iters * ops / run_time\n",
    "\n",
    "    #return cpu_rate/FLOPS_SCALE_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor.run_monitor_thread()\n",
    "SyMatMull(remote_torch, monitor)\n",
    "monitor.model_num = None\n",
    "monitor.QOS = 0\n",
    "monitor.QOS_list = [0]\n",
    "monitor.task = 'None'\n",
    "#time_data = monitor.stop_monitor_thread()\n",
    "#time_data.to_csv('data/microbench/{}/QOS_data_{}_{}.csv'.format(rpi_name,rpi_name,test_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def SyCholeskyInverse(torch_ref, monitor, n = 512, run_time = 300):\n",
    "    \n",
    "    \n",
    "    monitor.task = 'Cholesky_Inverse'\n",
    "    matrix = torch.randn(size=(n,n), dtype=torch.float32)\n",
    "    matrix = torch.mm(matrix, matrix.t()) + 1e-05 * torch.eye(3)\n",
    "    matrixIn = torch.cholesky(matrix)\n",
    "\n",
    "    matrixIn_pointer = matrixIn.send(duet, pointable=True)\n",
    "    print('Encryted matrixes sent')\n",
    "    \n",
    "    matrixOut = torch_ref.cholesky_inverse(matrixIn_pointer)\n",
    "\n",
    "    # runs the loop for the given amount of time\n",
    "    while (time.time()- run_time < start):\n",
    "        start_cholInv = time.time()\n",
    "        matrixOut = torch_ref.cholesky_inverse(matrixIn_pointer)\n",
    "        monitor.QOS = time.time() - start_cholInv\n",
    "        monitor.QOS_list.append(time.time() - start_cholInv)       \n",
    "\n",
    "    del matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SyInverse(torch_ref, monitor, n = 512, run_time = 300):\n",
    "    \n",
    "    # attempt to calculate flops\n",
    "    #FLOPS_SCALE_FACTOR = 100_000_000_000\n",
    "    # this represents the number of floating point computations per matrix mult\n",
    "    #matMulOps = (n**3) + ((n-1)*(n**2)) # // n^2*(n-1) additions and n^3 mults\n",
    "    #LUFactorizationOps = 2 * matMulOps\n",
    "    #lowerTriMatInv = n + n**2 + n**2 # n + n**2 multiplcationas and n**2 additions \n",
    "    #upperTriMatInv = 2*n + n**2\n",
    "    #matInvOps =   LUFactorizationOps  + LUInversion + matMulOps\n",
    "    # the inverse function is composed of an LU factorization (via a mat mul of three matrixes, L us a lower triangular matrix and U in an upper trainagular matrix) using     # the getrf routine and and the inverse of an LU-factored general matrix         \n",
    "    # determined by getri routine. The routines belong to the mkl::lapack namespace\n",
    "    # The getri function computes the inverse of both the triangle matrixes L and U by n^2 and then multiplies the inverses to come up witht the result\n",
    "\n",
    "    monitor.task = 'Inverse'\n",
    "    matrix = torch.randn(size=(n,n), dtype=torch.float32)\n",
    "    \n",
    "    matrix_pointer = matrix.send(duet, pointable=True)\n",
    "    print('Encryted matrixes sent')\n",
    "    \n",
    "    matrixOut = torch_ref.inverse(matrix_pointer)\n",
    "\n",
    "    #start = time.time()\n",
    "    #num_iters = 0\n",
    "\n",
    "    # runs the loop for the given amount of time\n",
    "    while (time.time()- run_time < start):\n",
    "        start_Inv = time.time()\n",
    "        matrixOut = torch_ref.inverse(matrix_pointer)\n",
    "        monitor.QOS = time.time() - start_Inv\n",
    "        monitor.QOS_list.append(time.time() - start_Inv) \n",
    "                \n",
    "        #num_iters += 1\n",
    "        #monitor.QOS = (num_iters * ops / ( time.time() - start )) / FLOPS_SCALE_FACTOR\n",
    "        #monitor.QOS_list.append(( num_iters * ops / ( time.time() - start ) ) / FLOPS_SCALE_FACTOR )\n",
    "\n",
    "    del matrix\n",
    "\n",
    "    # calculates the rate at which the CPU performs floating point computations\n",
    "    #cpu_rate = num_iters * ops / run_time\n",
    "\n",
    "    #return cpu_rate/FLOPS_SCALE_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor.run_monitor_thread()\n",
    "cpu_rate = SyCholeskyInverse(remote_torch, monitor)\n",
    "print(cpu_rate)\n",
    "monitor.model_num = None\n",
    "monitor.QOS = 0\n",
    "monitor.QOS_list = [0]\n",
    "monitor.task = 'None'\n",
    "#time_data = monitor.stop_monitor_thread()\n",
    "#time_data.to_csv('data/microbench/{}/QOS_data_{}_{}.csv'.format(rpi_name,rpi_name,test_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Array Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SyArrAccess(duet, monitor, arrLength = 100000, arrElementMax = 999999):\n",
    "    \n",
    "    monitor.task = 'Array Access'\n",
    "    \n",
    "    seed = int(time.time())\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    arr = np.empty(shape=(arrLength,2))\n",
    "    indexArr = np.arange(arrLength)\n",
    "    np.random.shuffle(indexArr)\n",
    "    \n",
    "    for i in range(arrLength):\n",
    "        arr[i,0] = np.random.randint(arrElementMax)\n",
    "        arr[i,1] = indexArr[i]\n",
    "    \n",
    "    arr = torch.FloatTensor(arr)\n",
    "    \n",
    "    arr = arr.tag(\"Send Tensor\")\n",
    "    arr = arr.describe(\"This 2D tensor contains randomized elements and access indexes for memory test with length \" + str(arrLength))\n",
    "\n",
    "    arr_ptr = arr.send(duet, pointable=True)\n",
    "    print('Tensor sent')\n",
    "    \n",
    "    searchFlag = True\n",
    "    print('Waiting to receive result')\n",
    "                   \n",
    "    while searchFlag == True:\n",
    "        for ptr in duet.store:\n",
    "            if ptr.tags[0] == 'Return Tensor':\n",
    "                ptr.request(reason = \"Why I ought'a ...\")\n",
    "                time.sleep(5)\n",
    "                squencedArrInfo = ptr.get()\n",
    "                searchFlag = False\n",
    "                print('Result received')  \n",
    "                   \n",
    "                return arr, arr_ptr, squencedArrInfo\n",
    "            \n",
    "def seqArrayCheck(arr, squencedArrInfo):\n",
    "    \n",
    "    squencedArrInfoNumpy = squencedArrInfo.numpy()\n",
    "    arrNumpy = arr[:,0].numpy()\n",
    "    indexArrNumpy = arr[:,1].numpy()\n",
    "\n",
    "    index = 0\n",
    "    squencedArr = squencedArrInfoNumpy[:,2]\n",
    "    \n",
    "    for i in indexArrNumpy:\n",
    "        if arrNumpy[int(i)] != squencedArr[index]:\n",
    "            print('Mismatch Error')\n",
    "            return False\n",
    "        index += 1\n",
    "    print(\"Arrays Match\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stamp_list = []\n",
    "\n",
    "monitor.run_monitor_thread()\n",
    "arr, arr_ptr, squencedArrInfo = SyArrAccess(duet, monitor, 100000, 999999)\n",
    "if seqArrayCheck(arr, squencedArrInfo) == True:\n",
    "    \n",
    "    squencedArrInfoNumpy = squencedArrInfo.numpy()\n",
    "    \n",
    "    for timeStamp in squencedArrInfoNumpy[:,0]:\n",
    "    \n",
    "        timeObj = time.localtime(timeStamp)\n",
    "        # creates time stamp Day-Month-Year Hour:Minute:Second\n",
    "        time_stamp_list.append(str(timeObj.tm_mday) + '-' + str(timeObj.tm_mon) + '-' + str(timeObj.tm_year) + ' ' + str(timeObj.tm_hour) + ':' + str(timeObj.tm_min) + ':' + str(timeObj.tm_sec))\n",
    "     \n",
    "    squencedArrInfoDF = pd.DataFrame.from_dict({'time_stamp':time_stamp_list, 'time':squencedArrInfoNumpy[:,0],'memory_access_time':squencedArrInfoNumpy[:,1]})\n",
    "    squencedArrInfoDF = squencedArrInfoDF.set_index('time_stamp')\n",
    "    \n",
    "    monitor.model_num = None\n",
    "    monitor.QOS = 0\n",
    "    monitor.QOS_list = [0]\n",
    "    monitor.task = 'None'\n",
    "    time_data = monitor.stop_monitor_thread()\n",
    "    time_data.to_csv('data/microbench/{}/QOS_data_{}_{}.csv'.format(rpi_name,rpi_name,test_name))\n",
    "    squencedArrInfoDF.to_csv('data/microbench/{}/ArrInfo_data_{}_{}.csv'.format(rpi_name,rpi_name,test_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
